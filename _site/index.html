<!DOCTYPE html>
<html lang="en-us">
  
  <head>
  <meta charset="UTF-8">
  <title>Geometric Consistency for Self-Supervised End-to-End Visual Odometry</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>


  <body>
    <section class="page-header">
  <img src="/assets/mila_logo_en.svg" style="width:25%;"> <br />
  <img src="/assets/udem_logo.png" style="width:15%;">
  <img src="/assets/rrc_logo.png" style="width:15%;">
  <img src="/assets/iiit_logo.png" style="width:15%;">
  <h1 class="project-name">Geometric Consistency for Self-Supervised End-to-End Visual Odometry</h1>
  <h2 class="project-tagline">Ganesh Iyer<sup>1*</sup>, J. Krishna Murthy<sup>2*</sup>, Gunshi Gupta<sup>1</sup>, K. Madhava Krishna<sup>1</sup>, Liam Paull<sup>2</sup> <br /> 1st International Workshop on Deep Learning for Visual SLAM, CVPR 2018 <br /></h2>
  <h3 class="project-affiliation"><sup>*</sup> The first two authors contributed equally to this work. <br /> <sup>1</sup> International Institute of Information Technology Hyderabad, India <br /> <sup>2</sup> MILA and DIRO, Universite de Montreal, Canada <br /></h3>
  <a href="http://openaccess.thecvf.com/CVPR2018_workshops/content_CVPR_2018/papers/w9/Iyer_Geometric_Consistency_for_CVPR_2018_paper.pdf" class="btn">Paper (PDF)</a>
  <a href="https://github.com/krrish94/CTCNet-release" class="btn">View on GitHub</a>
  <a href="https://github.com/krrish94/CTCNet-release/zipball/master" class="btn">Download .zip</a>
  <a href="https://github.com/krrish94/CTCNet-release/tarball/master" class="btn">Download .tar.gz</a>
  <a href="/assets/CTCNet_poster.pdf" class="btn">Poster (PDF)</a>
</section>


    <section class="main-content">
      
      <hr />

<h1 id="abstract">Abstract</h1>

<p>With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose “Composite Transformation Constraints (CTCs)”, that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.</p>

<hr />

<h1 id="intuition">Intuition</h1>

<p><img src="/assets/CTCNet_intuition.png" alt="alt text" title="Intuition of Composite Transformation Constraints" width="40%" /></p>

<p>We leverage the observation that compounded sequences of transformations over short timescales should be equivalent to a single transformation independently computed over longer timescales. This allows us to create Composite Transformation Constraints (CTCs) that can be used as supervisory signals for learning visual odometry.</p>

<hr />

<h1 id="ctcnet">CTCNet</h1>

<p><img src="/assets/CTCNet_architecture.png" alt="alt text" title="CTCNet architecture" /></p>

<p>End-to-end architecture: An example of Composite Transformation Constraints (CTCs) being applied to 4 successive input images. During training, two estimates are generated from the inputs: one for a sequential pairwise constraint and one for a CTC constraint. At test time, each frame is only fed into the network once to receive the output pose from the SE(3) layer.</p>

<hr />



      <footer class="site-footer">
  <span class="site-footer-credits"><a href="https://mila.quebec/en/">MILA: Montreal Institute for Learning Algorithms</a></span>
  <span class="site-footer-owner">This webpage is maintained by <a href="https://krrish94.github.io">Krishna Murthy</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/pietromenna/jekyll-cayman-theme">Cayman theme</a> by <a href="http://github.com/jasonlong">Jason Long</a>.</span>
</footer>


    </section>

  </body>
</html>
